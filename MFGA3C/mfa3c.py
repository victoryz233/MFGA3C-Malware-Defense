import argparse
import math
import os
import random
import threading
import time
import tensorflow as tf
import numpy as np
import multiprocessing as mp
import magent

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# Copies one set of variables to another.
# Used to set worker network parameters to those of global network.
def update_target_graph(from_scope, to_scope):
    from_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, from_scope)
    to_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, to_scope)
    # print("from_vars: ",from_vars)
    # print("to_vars: ",to_vars)
    op_holder = []
    for from_var,to_var in zip(from_vars,to_vars):
        op_holder.append(to_var.assign(from_var))
    #print("op_holder: ",op_holder)
    return op_holder

def linear_decay(epoch, x, y):
    min_v, max_v = y[0], y[-1]
    start, end = x[0], x[-1]

    if epoch == start:
        return min_v

    eps = min_v

    for i, x_i in enumerate(x):
        if epoch <= x_i:
            interval = (y[i] - y[i - 1]) / (x_i - x[i - 1])
            eps = interval * (epoch - x[i - 1]) + y[i - 1]
            break

    return eps

class Color:
    INFO = '\033[1;34m{}\033[0m'
    WARNING = '\033[1;33m{}\033[0m'
    ERROR = '\033[1;31m{}\033[0m'
class Buffer:
    def __init__(self):
        pass

    def push(self, **kwargs):
        raise NotImplementedError
class EpisodesBufferEntry:
    """Entry for episode buffer"""
    def __init__(self):
        self.views = []
        self.features = []
        self.actions = []
        self.rewards = []
        self.probs = []
        self.terminal = False

    def append(self, view, feature, action, reward, alive, probs=None):
        self.views.append(view.copy())
        self.features.append(feature.copy())
        self.actions.append(action)
        self.rewards.append(reward)
        if probs is not None:
            self.probs.append(probs)
        if not alive:
            self.terminal = True
class EpisodesBuffer(Buffer):
    """Replay buffer to store a whole episode for all agents
       one entry for one agent
    """
    def __init__(self, use_mean=False):
        super().__init__()
        self.buffer = {}
        self.use_mean = use_mean

    def push(self, **kwargs):
        view, feature = kwargs['state']
        acts = kwargs['acts']
        rewards = kwargs['rewards']
        alives = kwargs['alives']
        ids = kwargs['ids']

        if self.use_mean:
            probs = kwargs['prob']

        buffer = self.buffer
        index = np.random.permutation(len(view))

        for i in range(len(ids)):
            i = index[i]
            entry = buffer.get(ids[i])
            if entry is None:
                entry = EpisodesBufferEntry()
                buffer[ids[i]] = entry

            if self.use_mean:
                entry.append(view[i], feature[i], acts[i], rewards[i], alives[i], probs=probs[i])
            else:
                entry.append(view[i], feature[i], acts[i], rewards[i], alives[i])

    def reset(self):
        """ clear replay buffer """
        self.buffer = {}

    def episodes(self):
        """ get episodes """
        return self.buffer.values()
class A3C_Network:
    def __init__(self, scope, env, handle, value_coef=0.1, ent_coef=0.08, learning_rate=1e-4):
        self.scope = scope
        self.view_space = env.get_view_space(handle)
        self.feature_space = env.get_feature_space(handle)
        self.num_actions = env.get_action_space(handle)[0]

        self.view_buf = np.empty((1,) + self.view_space)
        self.feature_buf = np.empty((1,) + self.feature_space)
        self.action_buf = np.empty(1, dtype=np.int32)
        self.reward_buf = np.empty(1, dtype=np.float32)
        #MF
        self.replay_buffer = EpisodesBuffer(use_mean=True)

        self.learning_rate = learning_rate
        self.value_coef = value_coef
        self.ent_coef = ent_coef

        with tf.compat.v1.variable_scope(self.scope):
            # self.name_scope = tf.compat.v1.get_variable_scope().name
            self.input_view = tf.compat.v1.placeholder(tf.float32, (None,)+self.view_space)
            self.input_feature = tf.compat.v1.placeholder(tf.float32, (None,)+self.feature_space)

            #MF
            self.input_act_prob = tf.compat.v1.placeholder(tf.float32, (None, self.num_actions))

            self.action = tf.compat.v1.placeholder(tf.int32, [None])

            self.reward = tf.compat.v1.placeholder(tf.float32, [None])
            #reward = tf.compat.v1.placeholder(tf.int32, [None])
            hidden_size = [256]

            self.flatten_view = tf.reshape(self.input_view, [-1, np.prod([v for v in self.input_view.shape[1:]])])
            self.h_view = tf.compat.v1.layers.dense(self.flatten_view, units=hidden_size[0], activation=tf.nn.relu)
            self.h_emb = tf.compat.v1.layers.dense(self.input_feature, units=hidden_size[0], activation=tf.nn.relu)
            self.concat_layer = tf.concat([self.h_view, self.h_emb], axis=1)
            self.dense = tf.compat.v1.layers.dense(self.concat_layer, units=hidden_size[0]*2, activation=tf.nn.relu)

            self.policy = tf.compat.v1.layers.dense(self.dense/0.1, units=self.num_actions, activation=tf.nn.softmax)
            self.policy = tf.clip_by_value(self.policy, 1e-10, 1-1e-10)

            self.calc_action = tf.compat.v1.multinomial(tf.compat.v1.log(self.policy), 1)

            #MF
            self.emb_prob = tf.compat.v1.layers.dense(self.input_act_prob, units=64, activation=tf.nn.relu)
            self.dense_prob = tf.compat.v1.layers.dense(self.emb_prob, units=32, activation=tf.nn.relu)
            self.concat_layer = tf.concat([self.concat_layer, self.dense_prob], axis=1)
            self.dense = tf.compat.v1.layers.dense(self.concat_layer, units=hidden_size[0], activation=tf.nn.relu)


            self.value = tf.compat.v1.layers.dense(self.dense, units=1)
            self.value = tf.reshape(self.value, (-1,))

            if self.scope != 'global_attack' or self.scope != 'global_defense':
                self.actions = tf.compat.v1.placeholder(tf.int32, [None])
                self.action_mask = tf.compat.v1.one_hot(self.action, self.num_actions)
                self.advantage = tf.stop_gradient(self.reward - self.value)

                self.log_policy = tf.compat.v1.log(self.policy + 1e-6)
                self.log_prob = tf.compat.v1.reduce_sum(self.log_policy * self.action_mask, axis=1)

                self.pg_loss = -tf.reduce_mean(self.advantage * self.log_prob)
                self.vf_loss = self.value_coef * tf.reduce_mean(tf.square(self.reward - self.value))
                self.neg_entropy = self.ent_coef * tf.reduce_mean(tf.reduce_sum(self.policy * self.log_policy, axis=1))
                self.total_loss = self.pg_loss + self.vf_loss + self.neg_entropy
                self.reg_loss = self.neg_entropy
                self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)
                gradients = tf.compat.v1.gradients(self.total_loss, self.vars)
                #print("local_vars: ",self.vars)
                #gradients,variables= zip(*self.optimizer.compute_gradients(self.total_loss))
                #print("gradients: ",gradients)
                gradients, _ = tf.clip_by_global_norm(gradients, 5.0)

                #apply local vars into global vars
                #是否要考虑对global-defense进行参数传递？
                global_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'global_attack')
                #print("global_vars: ",global_vars)
                self.train_op = self.optimizer.apply_gradients(zip(gradients, global_vars))
                # train_op = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.total_loss)
                # self.train_op = train_op

    @property
    def vars(self):
        return tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.scope)


class SummaryObj:
    """
    Define a summary holder
    """
    def __init__(self, log_dir, log_name, n_group=1):
        self.name_set = set()
        self.gra = tf.Graph()
        self.n_group = n_group

        if not os.path.exists(log_dir):
            os.makedirs(log_dir)

        # sess_config = tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=False)
        # sess_config.gpu_options.allow_growth = True

        with self.gra.as_default():
            self.sess = tf.compat.v1.Session(graph=self.gra)
            self.train_writer = tf.compat.v1.summary.FileWriter(log_dir + "/" + log_name, graph=tf.compat.v1.get_default_graph())
            self.sess.run(tf.compat.v1.global_variables_initializer())

    def register(self, name_list):
        """Register summary operations with a list contains names for these operations

        Parameters
        ----------
        name_list: list, contains name whose type is str
        """

        with self.gra.as_default():
            for name in name_list:
                if name in self.name_set:
                    raise Exception("You cannot define different operations with same name: `{}`".format(name))
                self.name_set.add(name)
                setattr(self, name, [tf.compat.v1.placeholder(tf.float32, shape=None, name='Agent_{}_{}'.format(i, name))
                                     for i in range(self.n_group)])
                setattr(self, name + "_op", [tf.compat.v1.summary.scalar('Agent_{}_{}_op'.format(i, name), getattr(self, name)[i])
                                             for i in range(self.n_group)])

    def write(self, summary_dict, step):
        """Write summary related to a certain step

        Parameters
        ----------
        summary_dict: dict, summary value dict
        step: int, global step
        """

        assert isinstance(summary_dict, dict)

        for key, value in summary_dict.items():
            # print("key: {0} value:{1}".format(key, value))
            if key not in self.name_set:
                raise Exception("Undefined operation: `{}`".format(key))
            if isinstance(value, list):
               #print("key: {0} value:{1}".format(key, value))
                for i in range(self.n_group):
                    self.train_writer.add_summary(self.sess.run(getattr(self, key + "_op")[i], feed_dict={
                        getattr(self, key)[i]: value[i]}), global_step=step)
            else:
                self.train_writer.add_summary(self.sess.run(getattr(self, key + "_op")[0], feed_dict={
                        getattr(self, key)[0]: value}), global_step=step)


class Worker:
    def __init__(self, name, global_episodes, map_size=None, max_steps=None,
                 render_every=None, save_every=None,
             tau=None, log_name=None, log_dir=None, model_dir=None, train=False):
        self.name = "worker_" + str(name)
        # self.model_path = model_path
        # self.global_episodes = global_episodes
        # self.increment = self.global_episodes.assign_add(1)
        # global AVG_AGENT_REWARD

        #有待改进self.name部分
        self.local_AC = [A3C_Network(self.name+'_attack', env, handles[0]),
                         A3C_Network(self.name+'_defense', env, handles[1])]
        self.update_local_ops = update_target_graph('global_attack', self.name+"_attack")
        #self.update_local_defense_ops = update_target_graph('global_defense', self.name+"_defense")
        '''
        环境参数设定部分
        '''
        # self.actions = np.identity(a_size,dtype=bool).tolist()
        # 基本参数初始化
        self.env = magent.GridWorld('battle', map_size=map_size)
        # env.set_seed(0)
        #self.env = env
        # self.models = models
        self.max_steps = max_steps
        #self.handles = handles
        self.handles = self.env.get_handles()
        self.map_size = map_size
        self.render_every = render_every
        self.save_every = save_every
        self.model_dir = model_dir
        self.train = train
        self.tau = tau

    def flush_buffer(self, models, **kwargs):
        models.replay_buffer.push(**kwargs)

    #gamma和batch_data需要另外传入
    def trainBuff(self, sess, gamma):
        n = 0
        batch_data = self.local_AC[0].replay_buffer.episodes()
        #MF
        self.local_AC[0].replay_buffer = EpisodesBuffer(use_mean=True)

        for episode in batch_data:
            n += len(episode.rewards)

        self.local_AC[0].view_buf.resize((n,) + self.local_AC[0].view_space)
        self.local_AC[0].feature_buf.resize((n,) + self.local_AC[0].feature_space)
        self.local_AC[0].action_buf.resize(n)
        self.local_AC[0].reward_buf.resize(n)
        view, feature = self.local_AC[0].view_buf, self.local_AC[0].feature_buf
        action, reward = self.local_AC[0].action_buf, self.local_AC[0].reward_buf
        #MF
        act_prob_buff = np.zeros((n, self.local_AC[0].num_actions), dtype=np.float32)

        ct = 0
        # collect episodes from multiple separate buffers to a continuous buffer
        for episode in batch_data:
            v, f, a, r, prob = episode.views, episode.features, episode.actions, episode.rewards, episode.probs
            m = len(episode.rewards)

            assert len(prob) > 0

            r = np.array(r)

            keep = sess.run(self.local_AC[0].value, feed_dict={
                self.local_AC[0].input_view: [v[-1]],
                self.local_AC[0].input_feature: [f[-1]],
                self.local_AC[0].input_act_prob: [prob[-1]]
            })[0]

            for i in reversed(range(m)):
                keep = keep * gamma + r[i]
                r[i] = keep

            view[ct:ct + m] = v
            feature[ct:ct + m] = f
            action[ct:ct + m] = a
            reward[ct:ct + m] = r
            #MF
            act_prob_buff[ct:ct + m] = prob
            ct += m

        assert n == ct

        # train
        _, pg_loss, vf_loss, ent_loss, state_value = sess.run(
            [self.local_AC[0].train_op, self.local_AC[0].pg_loss, self.local_AC[0].vf_loss, self.local_AC[0].reg_loss, self.local_AC[0].value], feed_dict={
                self.local_AC[0].input_view: view,
                self.local_AC[0].input_feature: feature,
                #MF
                self.local_AC[0].input_act_prob: act_prob_buff,

                self.local_AC[0].action: action,
                self.local_AC[0].reward: reward,
            })
        if self.name == "worker_0":
            print('[*] PG_LOSS:', np.round(pg_loss, 6), '/ VF_LOSS:', np.round(vf_loss, 6), '/ ENT_LOSS:',
                    np.round(ent_loss), '/ Value:', np.mean(state_value))

    def generate_map(self, env, map_size, handles):
        """ generate a map, which consists of two squares of agents"""
        width = height = map_size
        init_num = map_size * map_size * 0.04
        gap = 3

        leftID = random.randint(0, 1)
        rightID = 1 - leftID

        # left
        n = init_num
        side = int(math.sqrt(n)) * 2
        pos = []
        for x in range(width // 2 - gap - side, width // 2 - gap - side + side, 2):
            for y in range((height - side) // 2, (height - side) // 2 + side, 2):
                pos.append([x, y, 0])
        env.add_agents(handles[leftID], method="custom", pos=pos)

        # right
        n = init_num
        side = int(math.sqrt(n)) * 2
        pos = []
        for x in range(width // 2 + gap, width // 2 + gap + side, 2):
            for y in range((height - side) // 2, (height - side) // 2 + side, 2):
                pos.append([x, y, 0])
        env.add_agents(handles[rightID], method="custom", pos=pos)

    def play(self, env, n_round, map_size, max_steps, handles, models, print_every, eps=1.0, render=False, train=False):
        """play a ground and train"""
        env.reset()
        self.generate_map(env, map_size, handles)

        step_ct = 0
        done = False

        n_group = len(handles)
        state = [None for _ in range(n_group)]
        acts = [None for _ in range(n_group)]
        ids = [None for _ in range(n_group)]

        alives = [None for _ in range(n_group)]
        rewards = [None for _ in range(n_group)]
        nums = [env.get_num(handle) for handle in handles]
        max_nums = nums.copy()

        loss = [None for _ in range(n_group)]
        eval_q = [None for _ in range(n_group)]
        n_action = [env.get_action_space(handles[0])[0], env.get_action_space(handles[1])[0]]
        if self.name == "worker_0": 
            print("\n\n[*] ROUND #{0}, EPS: {1:.2f} NUMBER: {2} Worker:{3}".format(n_round, eps, nums, self.name))
        mean_rewards = [[] for _ in range(n_group)]
        total_rewards = [[] for _ in range(n_group)]

        former_act_prob = [np.zeros((1, env.get_action_space(handles[0])[0])),
                           np.zeros((1, env.get_action_space(handles[1])[0]))]

        while not done and step_ct < max_steps:
            # take actions for every model
            for i in range(n_group):
                state[i] = list(env.get_observation(handles[i]))
                ids[i] = env.get_agent_id(handles[i])

            for i in range(n_group):
                former_act_prob[i] = np.tile(former_act_prob[i], (len(state[i][0]), 1))
                acts[i] = self.act(i, state=state[i], prob=former_act_prob[i], eps=eps)
            # print("acts: %s"%acts)
            for i in range(n_group):
                env.set_action(handles[i], acts[i])

            # simulate one step
            done = env.step()

            for i in range(n_group):
                rewards[i] = env.get_reward(handles[i])
                alives[i] = env.get_alive(handles[i])

            buffer = {
                'state': state[0], 'acts': acts[0], 'rewards': rewards[0],
                'alives': alives[0], 'ids': ids[0]
            }

            buffer['prob'] = former_act_prob[0]
            #if self.name == "worker_1":
             #   print("buffer: ",buffer)

            for i in range(n_group):
                former_act_prob[i] = np.mean(list(map(lambda x: np.eye(n_action[i])[x], acts[i])), axis=0,
                                             keepdims=True)

            if train:
                self.flush_buffer(models[0], **buffer)

            # stat info
            nums = [env.get_num(handle) for handle in handles]

            for i in range(n_group):
                sum_reward = sum(rewards[i])
                rewards[i] = sum_reward / nums[i]
                mean_rewards[i].append(rewards[i])
                total_rewards[i].append(sum_reward)

            if render:
                env.render()

            # clear dead agents
            env.clear_dead()

            info = {"Ave-Reward": np.round(rewards, decimals=6), "NUM": nums}

            step_ct += 1

            if step_ct % print_every == 0 and self.name == "worker_0":
                print("> step #{}, info: {}".format(step_ct, info))

        if train:
            self.trainBuff(sess, gamma=0.95)

        for i in range(n_group):
            mean_rewards[i] = sum(mean_rewards[i]) / len(mean_rewards[i])
            total_rewards[i] = sum(total_rewards[i])

        return max_nums, nums, mean_rewards, total_rewards

    def act(self, i, **kwargs):
        action = self.sess.run(self.local_AC[i].calc_action, {
            self.local_AC[i].input_view: kwargs['state'][0],
            self.local_AC[i].input_feature: kwargs['state'][1]
        })
        return action.astype(np.int32).reshape((-1,))

    def work(self, sess, coord=None):
# 与环境进行实际交互，填充经验池
#         episode_count = sess.run(self.global_episodes)
        # total_steps = 0
        log_dir = os.path.join(BASE_DIR, 'data/tmp/MFa3c/exp10_local_work10')
        print("Starting worker "+str(self.name))
        with sess.as_default(), sess.graph.as_default():
            while not coord.should_stop():
            #    sess.run(self.update_local_ops)


                if self.train:
                    self.summary = SummaryObj(log_name=self.name, log_dir=log_dir)

                    summary_items = ['ave_agent_reward', 'total_reward', 'kill', "Sum_Reward", "Kill_Sum"]
                    self.summary.register(summary_items)  # summary register
                    self.summary_items = summary_items

                    #Self-play 自我游戏
                    assert isinstance(sess, tf.compat.v1.Session)
                    assert self.local_AC[0].scope != self.local_AC[1].scope
                    self.sess = sess
                    l_vars, r_vars = self.local_AC[0].vars, self.local_AC[1].vars
                    assert len(l_vars) == len(r_vars)
                    self.sp_op = [tf.compat.v1.assign(r_vars[i], (1. - self.tau) * l_vars[i] + self.tau * r_vars[i])
                                  for i in range(len(l_vars))]

                    #if not os.path.exists(self.model_dir):
                        #os.makedirs(self.model_dir)
                #Epislion deacy
                for iteration in range(2000):
                    win_cnt = None
                    eps = linear_decay(iteration, [0, int(2000 * 0.8), 2000], [1, 0.2, 0.1])
                    # self.run(eps, k)
                    info = {'main': None, 'opponent': None}

                    # pass
                    info['main'] = {'ave_agent_reward': 0., 'total_reward': 0., 'kill': 0.}
                    info['opponent'] = {'ave_agent_reward': 0., 'total_reward': 0., 'kill': 0.}
                    # 调用self.play执行与环境进行交互，输出交互后的信息
                    max_nums, nums, agent_r_records, total_rewards = self.play(env=self.env, n_round=iteration,
                                                                               map_size=self.map_size,
                                                                               max_steps=self.max_steps,
                                                                               handles=self.handles,
                                                                               models=self.local_AC, print_every=50,
                                                                               eps=eps,
                                                                               render=(iteration + 1) % self.render_every if self.render_every > 0 else False,
                                                                               train=self.train)

                    for i, tag in enumerate(['main', 'opponent']):
                        info[tag]['total_reward'] = total_rewards[i]
                        info[tag]['kill'] = max_nums[i] - nums[1 - i]
                        info[tag]['ave_agent_reward'] = agent_r_records[i]

                    if self.train:
                        if self.name == "worker_0": 
                            print('\n[INFO] {}'.format(info['main']))

                        # if self.save_every and (iteration + 1) % self.save_every == 0:
                        if info['main']['total_reward'] > info['opponent']['total_reward']:
                            if self.name == "worker_0": 
                                print(Color.INFO.format('\n[INFO] Begin self-play Update ...'))
                            self.sess.run(self.sp_op)
                            if self.name == "worker_0": 
                                print(Color.INFO.format('[INFO] Self-play Updated!\n'))

                            # print(Color.INFO.format('[INFO] Saving model ...'))
                            # self.local_AC[0].save(self.model_dir + '-0', iteration)
                            # self.local_AC[1].save(self.model_dir + '-1', iteration)
                            AVG_AGENT_REWARD.append(info['main']['total_reward'])
                            self.summary.write(info['main'], iteration)
                    else:
                        if self.name == "worker_0": 
                            print('\n[INFO] {0} \n {1}'.format(info['main'], info['opponent']))
                        if info['main']['kill'] > info['opponent']['kill']:
                            win_cnt['main'] += 1
                        elif info['main']['kill'] < info['opponent']['kill']:
                            win_cnt['opponent'] += 1
                        else:
                            win_cnt['main'] += 1
                            win_cnt['opponent'] += 1
                    sess.run(self.update_local_ops)
                coord.request_stop()

if __name__ == "__main__":

    tf.compat.v1.disable_eager_execution()
    #用户输入参数
    parser = argparse.ArgumentParser()
    parser.add_argument('--save_every', type=int, default=10, help='decide the self-play update interval')
    parser.add_argument('--update_every', type=int, default=5,
                        help='decide the udpate interval for q-learning, optional')
    parser.add_argument('--n_round', type=int, default=2000, help='set the trainning round')
    parser.add_argument('--render', action='store_true', help='render or not (if true, will render every save)')
    parser.add_argument('--map_size', type=int, default=40,
                        help='set the size of map')  # then the amount of agents is 64
    parser.add_argument('--max_steps', type=int, default=400, help='set the max steps')
    args = parser.parse_args()
 
    AVG_AGENT_REWARD= []


    # model_dir = os.path.join(BASE_DIR, 'data/models/{}'.format(args.algo))

    with tf.compat.v1.device("/cpu:0"):
        # Initialize the environment
        env = magent.GridWorld('battle', map_size=args.map_size)
       # env.set_render_dir(os.path.join(BASE_DIR, 'examples/battle_model', 'build/render'))
        #获取组句柄、视图空间、特征空间、动作数
        handles = env.get_handles()
        # random.seed(0)#只能用一次，除非在循环里
        # np.random.seed(0)
        #env.set_seed(0)#对env.reset起到初始为固定值
        # tf.compat.v1.set_random_seed(0)#全局固定
        #trainable = False 代表此变量不参与更新
        global_episodes = tf.compat.v1.Variable(0, dtype=tf.int32, name='global_episodes', trainable=False)
        #定义全局网络
        master_network = [A3C_Network("global_attack", env, handles[0]), A3C_Network("global_defense", env, handles[1])]

        num_workers = mp.cpu_count()
        workers = []
        for i in range(10):
            print("work_{} initialize!".format(i))
            #print("map_size: ",args.map_size)
            workers.append(Worker(i, global_episodes,
                                  map_size=args.map_size, max_steps=args.max_steps, render_every=args.save_every if args.render else 0,
                                  save_every=args.save_every, tau=0.01, train=True))

    with tf.compat.v1.Session() as sess:
        coord = tf.compat.v1.train.Coordinator()
        sess.run(tf.compat.v1.global_variables_initializer())

        worker_threads = []
        for worker in workers:
            #需要传入worker参数
            worker_work = lambda:worker.work(sess,coord)
            t = threading.Thread(target=(worker_work))
            t.start()
            time.sleep(0.5)
            worker_threads.append(t)
        coord.join(worker_threads)

